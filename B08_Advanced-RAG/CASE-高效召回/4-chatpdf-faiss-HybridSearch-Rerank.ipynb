{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9739dd6-9461-4d0e-ac86-8f7f4d4e2822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import Tongyi\n",
    "from langchain_core.documents import Document\n",
    "from rank_bm25 import BM25Okapi\n",
    "from modelscope import AutoModelForSequenceClassification, AutoTokenizer, snapshot_download\n",
    "from typing import List, Tuple, Set\n",
    "import jieba\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "DASHSCOPE_API_KEY = os.getenv('DASHSCOPE_API_KEY')\n",
    "if not DASHSCOPE_API_KEY:\n",
    "    raise ValueError(\"请设置环境变量 DASHSCOPE_API_KEY\")\n",
    "\n",
    "def extract_text_with_page_numbers(pdf) -> Tuple[str, List[int]]:\n",
    "    \"\"\"从PDF中提取文本并记录每行文本对应的页码\"\"\"\n",
    "    text = \"\"\n",
    "    page_numbers = []\n",
    "\n",
    "    for page_number, page in enumerate(pdf.pages, start=1):\n",
    "        extracted_text = page.extract_text()\n",
    "        if extracted_text:\n",
    "            text += extracted_text\n",
    "            page_numbers.extend([page_number] * len(extracted_text.split(\"\\n\")))\n",
    "        else:\n",
    "            print(f\"No text found on page {page_number}.\")\n",
    "\n",
    "    return text, page_numbers\n",
    "\n",
    "def tokenize_chinese(text: str) -> List[str]:\n",
    "    \"\"\"中文分词\"\"\"\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "class Reranker:\n",
    "    \"\"\"基于ModelScope的Rerank模型\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-reranker-base\", cache_dir: str = \"./models\"):\n",
    "        \"\"\"\n",
    "        初始化Reranker\n",
    "\n",
    "        参数:\n",
    "            model_name: ModelScope上的模型名称\n",
    "                - BAAI/bge-reranker-base (轻量级，推荐)\n",
    "                - BAAI/bge-reranker-large (效果更好，但更慢)\n",
    "            cache_dir: 模型缓存目录，默认为当前目录下的 ./models\n",
    "        \"\"\"\n",
    "        print(f\"正在加载Rerank模型: {model_name}\")\n",
    "        print(f\"模型缓存目录: {cache_dir}\")\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "        # 先下载模型到指定目录\n",
    "        model_dir = snapshot_download(model_name, cache_dir=cache_dir)\n",
    "        print(f\"模型已下载到: {model_dir}\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        self.model.eval()\n",
    "\n",
    "        # 使用GPU如果可用\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Rerank模型已加载，使用设备: {self.device}\")\n",
    "\n",
    "    def rerank(self, query: str, documents: List[Document], top_k: int = None) -> List[Document]:\n",
    "        \"\"\"\n",
    "        对文档进行重排序\n",
    "\n",
    "        参数:\n",
    "            query: 查询文本\n",
    "            documents: 待排序的文档列表\n",
    "            top_k: 返回前k个结果，None表示返回全部\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "\n",
    "        # 构建query-document对\n",
    "        pairs = [[query, doc.page_content] for doc in documents]\n",
    "\n",
    "        # 批量计算分数\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(\n",
    "                pairs,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            scores = self.model(**inputs).logits.squeeze(-1).cpu().tolist()\n",
    "\n",
    "        # 如果只有一个文档，scores可能是float而不是list\n",
    "        if isinstance(scores, float):\n",
    "            scores = [scores]\n",
    "\n",
    "        # 为每个文档添加rerank分数\n",
    "        scored_docs = list(zip(documents, scores))\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 更新metadata中的分数\n",
    "        results = []\n",
    "        for doc, score in scored_docs:\n",
    "            doc.metadata[\"rerank_score\"] = score\n",
    "            results.append(doc)\n",
    "\n",
    "        if top_k:\n",
    "            results = results[:top_k]\n",
    "\n",
    "        return results\n",
    "\n",
    "class HybridRetriever:\n",
    "    \"\"\"混合检索器: BM25 + Vector\"\"\"\n",
    "\n",
    "    def __init__(self, chunks: List[str], vectorstore: FAISS, alpha: float = 0.5):\n",
    "        self.chunks = chunks\n",
    "        self.vectorstore = vectorstore\n",
    "        self.alpha = alpha\n",
    "\n",
    "        tokenized_chunks = [tokenize_chinese(chunk) for chunk in chunks]\n",
    "        self.bm25 = BM25Okapi(tokenized_chunks)\n",
    "        self.chunk_to_idx = {chunk: idx for idx, chunk in enumerate(chunks)}\n",
    "\n",
    "    def search(self, query: str, k: int = 4) -> List[Document]:\n",
    "        \"\"\"执行混合检索\"\"\"\n",
    "        tokenized_query = tokenize_chinese(query)\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "\n",
    "        max_bm25 = max(bm25_scores) if max(bm25_scores) > 0 else 1\n",
    "        bm25_scores_normalized = [s / max_bm25 for s in bm25_scores]\n",
    "\n",
    "        vector_results = self.vectorstore.similarity_search_with_score(query, k=len(self.chunks))\n",
    "\n",
    "        vector_scores = {}\n",
    "        max_distance = max(score for _, score in vector_results) if vector_results else 1\n",
    "        for doc, distance in vector_results:\n",
    "            idx = self.chunk_to_idx.get(doc.page_content)\n",
    "            if idx is not None:\n",
    "                vector_scores[idx] = 1 - (distance / max_distance) if max_distance > 0 else 0\n",
    "\n",
    "        hybrid_scores = []\n",
    "        for idx in range(len(self.chunks)):\n",
    "            bm25_score = bm25_scores_normalized[idx]\n",
    "            vector_score = vector_scores.get(idx, 0)\n",
    "            combined = self.alpha * vector_score + (1 - self.alpha) * bm25_score\n",
    "            hybrid_scores.append((idx, combined))\n",
    "\n",
    "        hybrid_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_k = hybrid_scores[:k]\n",
    "\n",
    "        results = []\n",
    "        for idx, score in top_k:\n",
    "            doc = Document(page_content=self.chunks[idx], metadata={\"hybrid_score\": score})\n",
    "            results.append(doc)\n",
    "\n",
    "        return results\n",
    "\n",
    "def process_text_with_splitter(text: str, page_numbers: List[int], save_path: str = None) -> Tuple[FAISS, List[str]]:\n",
    "    \"\"\"处理文本并创建向量存储\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"文本被分割成 {len(chunks)} 个块。\")\n",
    "\n",
    "    embeddings = DashScopeEmbeddings(\n",
    "        model=\"text-embedding-v1\",\n",
    "        dashscope_api_key=DASHSCOPE_API_KEY,\n",
    "    )\n",
    "\n",
    "    knowledgeBase = FAISS.from_texts(chunks, embeddings)\n",
    "    print(\"已从文本块创建知识库。\")\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    page_info = {}\n",
    "    for chunk in chunks:\n",
    "        start_idx = text.find(chunk[:100])\n",
    "        if start_idx == -1:\n",
    "            for i, line in enumerate(lines):\n",
    "                if chunk.startswith(line[:min(50, len(line))]):\n",
    "                    start_idx = i\n",
    "                    break\n",
    "            if start_idx == -1:\n",
    "                for i, line in enumerate(lines):\n",
    "                    if line and line in chunk:\n",
    "                        start_idx = text.find(line)\n",
    "                        break\n",
    "        if start_idx != -1:\n",
    "            line_count = text[:start_idx].count(\"\\n\")\n",
    "            if line_count < len(page_numbers):\n",
    "                page_info[chunk] = page_numbers[line_count]\n",
    "            else:\n",
    "                page_info[chunk] = page_numbers[-1] if page_numbers else 1\n",
    "        else:\n",
    "            page_info[chunk] = -1\n",
    "    knowledgeBase.page_info = page_info\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        knowledgeBase.save_local(save_path)\n",
    "        print(f\"向量数据库已保存到: {save_path}\")\n",
    "\n",
    "        with open(os.path.join(save_path, \"page_info.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(page_info, f)\n",
    "\n",
    "        with open(os.path.join(save_path, \"chunks.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(chunks, f)\n",
    "\n",
    "    return knowledgeBase, chunks\n",
    "\n",
    "def load_knowledge_base(load_path: str, embeddings=None) -> Tuple[FAISS, List[str]]:\n",
    "    \"\"\"从磁盘加载向量数据库\"\"\"\n",
    "    if embeddings is None:\n",
    "        embeddings = DashScopeEmbeddings(\n",
    "            model=\"text-embedding-v1\",\n",
    "            dashscope_api_key=DASHSCOPE_API_KEY,\n",
    "        )\n",
    "\n",
    "    knowledgeBase = FAISS.load_local(load_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(f\"向量数据库已从 {load_path} 加载。\")\n",
    "\n",
    "    page_info_path = os.path.join(load_path, \"page_info.pkl\")\n",
    "    if os.path.exists(page_info_path):\n",
    "        with open(page_info_path, \"rb\") as f:\n",
    "            page_info = pickle.load(f)\n",
    "        knowledgeBase.page_info = page_info\n",
    "\n",
    "    chunks_path = os.path.join(load_path, \"chunks.pkl\")\n",
    "    chunks = []\n",
    "    if os.path.exists(chunks_path):\n",
    "        with open(chunks_path, \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "\n",
    "    return knowledgeBase, chunks\n",
    "\n",
    "def generate_multi_queries(query: str, llm, num_queries: int = 3) -> List[str]:\n",
    "    \"\"\"使用LLM生成多个查询变体\"\"\"\n",
    "    prompt = f\"\"\"你是一个AI助手，负责生成多个不同视角的搜索查询。\n",
    "给定一个用户问题，生成{num_queries}个不同但相关的查询，以帮助检索更全面的信息。\n",
    "\n",
    "原始问题: {query}\n",
    "\n",
    "请直接输出{num_queries}个查询，每行一个，不要编号和其他内容:\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    queries = [q.strip() for q in response.strip().split('\\n') if q.strip()]\n",
    "    return [query] + queries[:num_queries]\n",
    "\n",
    "def hybrid_multi_query_search_with_rerank(\n",
    "    query: str,\n",
    "    hybrid_retriever: HybridRetriever,\n",
    "    reranker: Reranker,\n",
    "    llm,\n",
    "    initial_k: int = 10,\n",
    "    final_k: int = 4\n",
    ") -> List[Document]:\n",
    "    \"\"\"混合检索 + 多查询 + Rerank\"\"\"\n",
    "    queries = generate_multi_queries(query, llm)\n",
    "    print(f\"生成的查询变体: {queries}\")\n",
    "\n",
    "    # 第一阶段: 多查询混合检索，获取更多候选\n",
    "    seen_contents = set()\n",
    "    candidate_docs = []\n",
    "\n",
    "    for q in queries:\n",
    "        docs = hybrid_retriever.search(q, k=initial_k)\n",
    "        for doc in docs:\n",
    "            if doc.page_content not in seen_contents:\n",
    "                seen_contents.add(doc.page_content)\n",
    "                candidate_docs.append(doc)\n",
    "\n",
    "    print(f\"初步召回 {len(candidate_docs)} 个候选文档\")\n",
    "\n",
    "    # 第二阶段: Rerank精排\n",
    "    reranked_docs = reranker.rerank(query, candidate_docs, top_k=final_k)\n",
    "    print(f\"Rerank后保留 {len(reranked_docs)} 个文档\")\n",
    "\n",
    "    return reranked_docs\n",
    "\n",
    "def process_query(\n",
    "    query: str,\n",
    "    hybrid_retriever: HybridRetriever,\n",
    "    reranker: Reranker,\n",
    "    vectorstore: FAISS,\n",
    "    llm\n",
    ") -> Tuple[str, Set]:\n",
    "    \"\"\"处理查询并返回回答\"\"\"\n",
    "    docs = hybrid_multi_query_search_with_rerank(query, hybrid_retriever, reranker, llm)\n",
    "\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    prompt = f\"\"\"根据以下上下文回答问题:\n",
    "\n",
    "{context}\n",
    "\n",
    "问题: {query}\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    unique_pages = set()\n",
    "    for doc in docs:\n",
    "        source_page = vectorstore.page_info.get(doc.page_content.strip(), \"未知\")\n",
    "        unique_pages.add(source_page)\n",
    "\n",
    "    return response, unique_pages\n",
    "\n",
    "def main():\n",
    "    pdf_path = './浦发上海浦东发展银行西安分行个金客户经理考核办法.pdf'\n",
    "    vector_db_path = './vector_db_hybrid'\n",
    "\n",
    "    if os.path.exists(vector_db_path) and os.path.isdir(vector_db_path):\n",
    "        print(f\"发现现有向量数据库: {vector_db_path}\")\n",
    "        embeddings = DashScopeEmbeddings(\n",
    "            model=\"text-embedding-v1\",\n",
    "            dashscope_api_key=DASHSCOPE_API_KEY,\n",
    "        )\n",
    "        knowledgeBase, chunks = load_knowledge_base(vector_db_path, embeddings)\n",
    "    else:\n",
    "        print(f\"未找到向量数据库，将从PDF创建新的向量数据库\")\n",
    "        pdf_reader = PdfReader(pdf_path)\n",
    "        text, page_numbers = extract_text_with_page_numbers(pdf_reader)\n",
    "        print(f\"提取的文本长度: {len(text)} 个字符。\")\n",
    "        knowledgeBase, chunks = process_text_with_splitter(text, page_numbers, save_path=vector_db_path)\n",
    "\n",
    "    # 创建混合检索器\n",
    "    hybrid_retriever = HybridRetriever(chunks, knowledgeBase, alpha=0.5)\n",
    "    print(\"混合检索器已创建 (BM25 + Vector)\")\n",
    "\n",
    "    # 创建Reranker (使用ModelScope上的轻量级模型)\n",
    "    reranker = Reranker(model_name=\"BAAI/bge-reranker-base\")\n",
    "\n",
    "    llm = Tongyi(model_name=\"deepseek-v3\", dashscope_api_key=DASHSCOPE_API_KEY)\n",
    "\n",
    "    queries = [\n",
    "        \"客户经理被投诉了，投诉一次扣多少分\",\n",
    "        # \"客户经理每年评聘申报时间是怎样的？\",\n",
    "        # \"客户经理的考核标准是什么？\"\n",
    "    ]\n",
    "\n",
    "    for query in queries:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"查询: {query}\")\n",
    "\n",
    "        response, unique_pages = process_query(query, hybrid_retriever, reranker, knowledgeBase, llm)\n",
    "\n",
    "        print(\"\\n回答:\")\n",
    "        print(response)\n",
    "\n",
    "        print(\"\\n来源页码:\")\n",
    "        for page in sorted(unique_pages):\n",
    "            print(f\"- 第 {page} 页\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
